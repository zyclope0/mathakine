#!/usr/bin/env python3
"""
Benchmark pour comparer les performances de get_challenges_list.
Utilis√© pour valider l'optimisation PERF-3.2.
"""

import time
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from app.db.base import SessionLocal
from app.models.logic_challenge import LogicChallenge
from sqlalchemy import func
from app.core.logging_config import get_logger

logger = get_logger(__name__)

def benchmark_old_method(db):
    """Ancienne m√©thode : 2 requ√™tes"""
    start = time.time()
    
    # Requ√™te 1 : Liste
    challenges = db.query(LogicChallenge).filter(LogicChallenge.is_active == True).limit(20).all()
    
    # Requ√™te 2 : Count
    total = db.query(LogicChallenge).filter(LogicChallenge.is_active == True).count()
    
    elapsed = time.time() - start
    return elapsed, len(challenges), total

def benchmark_new_method(db):
    """Nouvelle m√©thode : 1 requ√™te avec COUNT OVER"""
    start = time.time()
    
    # Une seule requ√™te avec COUNT(*) OVER()
    query = db.query(
        LogicChallenge,
        func.count().over().label('total')
    ).filter(LogicChallenge.is_active == True).limit(20)
    
    results = query.all()
    challenges = [challenge for challenge, _ in results]
    total = results[0][1] if results else 0
    
    elapsed = time.time() - start
    return elapsed, len(challenges), total

def main():
    """Point d'entr√©e principal"""
    print("=" * 80)
    print("‚ö° BENCHMARK: get_challenges_list")
    print("=" * 80)
    print()
    
    db = SessionLocal()
    
    try:
        # Warmup
        db.query(LogicChallenge).first()
        
        # Benchmark ancienne m√©thode
        print("üîÑ Test ancienne m√©thode (2 requ√™tes)...")
        old_times = []
        for _ in range(10):
            elapsed, _, _ = benchmark_old_method(db)
            old_times.append(elapsed)
        old_avg = sum(old_times) / len(old_times)
        old_min = min(old_times)
        old_max = max(old_times)
        
        # Benchmark nouvelle m√©thode
        print("üîÑ Test nouvelle m√©thode (1 requ√™te avec COUNT OVER)...")
        new_times = []
        for _ in range(10):
            elapsed, _, _ = benchmark_new_method(db)
            new_times.append(elapsed)
        new_avg = sum(new_times) / len(new_times)
        new_min = min(new_times)
        new_max = max(new_times)
        
        # R√©sultats
        print()
        print("=" * 80)
        print("üìä R√âSULTATS")
        print("=" * 80)
        print(f"Ancienne m√©thode (2 requ√™tes):")
        print(f"  Moyenne: {old_avg*1000:.2f}ms")
        print(f"  Min: {old_min*1000:.2f}ms")
        print(f"  Max: {old_max*1000:.2f}ms")
        print()
        print(f"Nouvelle m√©thode (1 requ√™te):")
        print(f"  Moyenne: {new_avg*1000:.2f}ms")
        print(f"  Min: {new_min*1000:.2f}ms")
        print(f"  Max: {new_max*1000:.2f}ms")
        print()
        
        improvement = ((old_avg - new_avg) / old_avg) * 100
        print(f"üí° Am√©lioration: {improvement:.1f}%")
        
        if improvement > 0:
            print("‚úÖ La nouvelle m√©thode est plus rapide")
        else:
            print("‚ö†Ô∏è  La nouvelle m√©thode n'est pas plus rapide (v√©rifier l'impl√©mentation)")
        
        return 0
        
    except Exception as e:
        logger.error(f"Erreur: {e}")
        import traceback
        logger.debug(traceback.format_exc())
        return 1
    finally:
        db.close()

if __name__ == "__main__":
    sys.exit(main())

